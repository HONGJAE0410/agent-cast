"""Searcher Agent for web crawling and information collection."""

import time
import json
import re
import requests
from datetime import datetime, timedelta, timezone
from urllib.parse import urljoin

from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from webdriver_manager.chrome import ChromeDriverManager
from selenium.common.exceptions import StaleElementReferenceException, NoSuchElementException, TimeoutException
from selenium.webdriver.support.ui import WebDriverWait, Select
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.common.action_chains import ActionChains

from bs4 import BeautifulSoup

from .base_agent import BaseAgent
from ..state import WorkflowState

from .base_agent import BaseAgent
from ..state import WorkflowState

class WebSearcher:
    def __init__(self, perplexity_api_key: str = None):
        self.driver = None
        self.perplexity_api_key = perplexity_api_key or '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'
        self.setup_driver()
    
    def setup_driver(self):
        """WebDriver ì„¤ì •"""
        print("WebDriver ì„¤ì •ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
        chrome_options = Options()
        # chrome_options.add_argument("--headless")
        chrome_options.add_argument("--start-maximized")
        chrome_options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36")
        chrome_options.add_argument("--disable-blink-features=AutomationControlled")
        chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
        chrome_options.add_experimental_option('useAutomationExtension', False)
        
        service = Service(ChromeDriverManager().install())
        self.driver = webdriver.Chrome(service=service, options=chrome_options)
        print("WebDriver ì„¤ì •ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    def close_driver(self):
        """WebDriver ì¢…ë£Œ"""
        if self.driver:
            self.driver.quit()
            print("WebDriverë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
    
    def crawl_pytorch_kr(self):
        """íŒŒì´í† ì¹˜ í•œêµ­ ì‚¬ìš©ì ëª¨ì„ í¬ë¡¤ë§"""
        print("\n=== íŒŒì´í† ì¹˜ í•œêµ­ ì‚¬ìš©ì ëª¨ì„ í¬ë¡¤ë§ ì‹œì‘ ===")
        
        URL = "https://discuss.pytorch.kr/c/news"
        self.driver.get(URL)
        print(f"'{URL}' í˜ì´ì§€ë¡œ ì´ë™í•©ë‹ˆë‹¤.")
        time.sleep(3)

        one_week_ago = datetime.now() - timedelta(days=7)
        post_info = {}

        print("\nìµœì‹  ê²Œì‹œê¸€ ìˆ˜ì§‘ì„ ì‹œì‘í•©ë‹ˆë‹¤ (ìŠ¤í¬ë¡¤ ë‹¤ìš´)...")
        while True:
            topic_list_items = self.driver.find_elements(By.CSS_SELECTOR, "tbody.topic-list-body tr.topic-list-item")
            
            if not topic_list_items:
                print("ê²Œì‹œê¸€ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                break

            last_post_date = None
            
            for item in topic_list_items:
                try:
                    date_span = item.find_element(By.CSS_SELECTOR, "span.relative-date")
                    post_timestamp_ms = int(date_span.get_attribute("data-time"))
                    post_date = datetime.fromtimestamp(post_timestamp_ms / 1000)
                    
                    last_post_date = post_date
                    
                    if post_date >= one_week_ago:
                        link_element = item.find_element(By.CSS_SELECTOR, "a.title")
                        link = link_element.get_attribute("href")
                        if link not in post_info:
                            post_info[link] = post_date
                    
                except (StaleElementReferenceException, NoSuchElementException):
                    continue

            print(f"í˜„ì¬ê¹Œì§€ ìˆ˜ì§‘ëœ ë§í¬ ìˆ˜: {len(post_info)}")

            if last_post_date and last_post_date < one_week_ago:
                print("ì¼ì£¼ì¼ ì´ì „ ê²Œì‹œê¸€ì— ë„ë‹¬í•˜ì—¬ ìŠ¤í¬ë¡¤ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
                break
                
            self.driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(2)

        crawled_data = []
        print(f"\nì´ {len(post_info)}ê°œì˜ ìµœì‹  ë‰´ìŠ¤ì— ëŒ€í•œ í¬ë¡¤ë§ì„ ì‹œì‘í•©ë‹ˆë‹¤.")

        for link, post_date in post_info.items():
            try:
                self.driver.get(link)
                time.sleep(2)

                title_element = self.driver.find_element(By.CSS_SELECTOR, "a.fancy-title")
                title = title_element.text

                post_content_element = self.driver.find_element(By.CSS_SELECTOR, "article#post_1 div.cooked")
                content_text = post_content_element.text
                
                formatted_date = post_date.strftime("%Y-%m-%d")
                
                crawled_data.append({
                    "title": title,
                    "url": link,
                    "content": content_text,
                    "date": formatted_date,
                    "source": "íŒŒì´í† ì¹˜ í•œêµ­ ì‚¬ìš©ì ëª¨ì„",
                    "category": "ê¸°ìˆ "
                })
                
                print(f"âœ… ì œëª©: {title}")
                print(f"   ë‚ ì§œ: {formatted_date}")
                print(f"   URL: {link}\n")
                
            except Exception as e:
                print(f"âŒ URL {link} í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

        print(f"íŒŒì´í† ì¹˜ í•œêµ­ ì‚¬ìš©ì ëª¨ì„ í¬ë¡¤ë§ ì™„ë£Œ! ì´ {len(crawled_data)}ê°œì˜ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤.")
        return crawled_data

    def crawl_techcrunch(self):
        """TechCrunch í¬ë¡¤ë§"""
        print("\n=== TechCrunch í¬ë¡¤ë§ ì‹œì‘ ===")
        
        URL = "https://techcrunch.com/category/artificial-intelligence/"
        self.driver.get(URL)
        time.sleep(3) 

        one_week_ago = datetime.now(timezone.utc) - timedelta(days=7)
        article_data = {} 
        processed_articles_on_page = set()

        while True:
            articles = self.driver.find_elements(By.CSS_SELECTOR, "li.wp-block-post")
            
            stop_crawling = False
            
            for article in articles:
                try:
                    link_element = article.find_element(By.CSS_SELECTOR, "h3 a")
                    link = link_element.get_attribute("href")
                    
                    if link in processed_articles_on_page:
                        continue
                    
                    processed_articles_on_page.add(link)

                    date_element = article.find_element(By.CSS_SELECTOR, "time")
                    date_str_iso = date_element.get_attribute("datetime")
                    article_date = datetime.fromisoformat(date_str_iso)
                    
                    if article_date < one_week_ago:
                        print("ì¼ì£¼ì¼ ì´ì „ ê¸°ì‚¬ì— ë„ë‹¬í•˜ì—¬ ìˆ˜ì§‘ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
                        stop_crawling = True
                        break
                    
                    formatted_date = article_date.strftime('%Y-%m-%d')
                    article_data[link] = formatted_date
                    
                except (StaleElementReferenceException, NoSuchElementException):
                    continue
            
            print(f"í˜„ì¬ê¹Œì§€ ìˆ˜ì§‘ëœ ë§í¬ ìˆ˜: {len(article_data)}")
            
            if stop_crawling:
                break

            try:
                next_button = self.driver.find_element(By.CSS_SELECTOR, "a.wp-block-query-pagination-next")
                print("ë‹¤ìŒ í˜ì´ì§€ë¡œ ì´ë™í•©ë‹ˆë‹¤...")
                self.driver.execute_script("arguments[0].click();", next_button)
                processed_articles_on_page.clear()
                time.sleep(3)
            except NoSuchElementException:
                print("'Next' ë²„íŠ¼ì„ ì°¾ì„ ìˆ˜ ì—†ì–´ í¬ë¡¤ë§ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                break

        crawled_data = []
        print(f"\nì´ {len(article_data)}ê°œì˜ ìµœì‹  ê¸°ì‚¬ì— ëŒ€í•œ í¬ë¡¤ë§ì„ ì‹œì‘í•©ë‹ˆë‹¤.")

        for link, date_str in article_data.items():
            try:
                self.driver.get(link)
                time.sleep(2)

                title = self.driver.find_element(By.CSS_SELECTOR, "h1.wp-block-post-title").text
                content = self.driver.find_element(By.CSS_SELECTOR, "div.entry-content").text
                
                crawled_data.append({
                    "title": title,
                    "url": link,
                    "content": content,
                    "date": date_str,
                    "source": "TechCrunch",
                    "category": "ì‚°ì—…"
                })
                
                print(f"âœ… ì œëª©: {title}")
                print(f"   ë‚ ì§œ: {date_str}")
                print(f"   (URL: {link})\n")
                
            except Exception as e:
                print(f"âŒ URL {link} í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

        print(f"TechCrunch í¬ë¡¤ë§ ì™„ë£Œ! ì´ {len(crawled_data)}ê°œì˜ ê¸°ì‚¬ë¥¼ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤.")
        return crawled_data

    def crawl_huggingface(self):
        """HuggingFace Trending Papers í¬ë¡¤ë§"""
        print("\n=== HuggingFace Trending Papers í¬ë¡¤ë§ ì‹œì‘ ===")
        
        START_URL = "https://huggingface.co/papers/trending"
        self.driver.get(START_URL)

        try:
            WebDriverWait(self.driver, 15).until(EC.presence_of_element_located((By.TAG_NAME, "article")))
            print("í˜ì´ì§€ ë° ë…¼ë¬¸ ëª©ë¡ ë¡œë“œ ì™„ë£Œ.")
        except TimeoutException:
            print("ì˜¤ë¥˜: í˜ì´ì§€ë¥¼ ë¡œë“œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
            return []

        one_week_ago = datetime.now() - timedelta(days=7)
        paper_data = {}
        last_processed_count = 0

        while True:
            papers = self.driver.find_elements(By.TAG_NAME, "article")
            
            if len(papers) == last_processed_count:
                print("ìƒˆë¡œìš´ ë…¼ë¬¸ì´ ë” ì´ìƒ ë¡œë“œë˜ì§€ ì•Šì•„ ìŠ¤í¬ë¡¤ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
                break

            stop_crawling = False
            for paper in papers[last_processed_count:]:
                try:
                    all_spans = paper.find_elements(By.TAG_NAME, "span")
                    paper_date_found = False
                    for span in all_spans:
                        try:
                            date_text = span.text.replace("Published on ", "").strip()
                            paper_date = datetime.strptime(date_text, "%b %d, %Y")
                            paper_date_found = True
                            break 
                        except ValueError:
                            continue
                    
                    if paper_date_found:
                        if paper_date < one_week_ago:
                            stop_crawling = True
                            break
                        
                        link_element = paper.find_element(By.CSS_SELECTOR, "h3 a")
                        full_link = link_element.get_attribute("href")
                        paper_data[full_link] = paper_date.strftime('%Y-%m-%d')

                except Exception:
                    continue
            
            print(f"í˜„ì¬ê¹Œì§€ ìˆ˜ì§‘ëœ ë…¼ë¬¸ ìˆ˜: {len(paper_data)}")
            
            if stop_crawling:
                print("ì¼ì£¼ì¼ ì´ì „ ë…¼ë¬¸ì— ë„ë‹¬í•˜ì—¬ ìˆ˜ì§‘ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
                break
            
            last_processed_count = len(papers)
            self.driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(3)

        crawled_data = []
        print(f"\nì´ {len(paper_data)}ê°œì˜ ë…¼ë¬¸ì„ í¬ë¡¤ë§í•©ë‹ˆë‹¤.")

        for link, date_str in paper_data.items():
            try:
                self.driver.get(link)
                abstract_header = WebDriverWait(self.driver, 10).until(
                    EC.presence_of_element_located((By.XPATH, "//h2[text()='Abstract']"))
                )
                
                title = self.driver.find_element(By.CSS_SELECTOR, "h1").text
                abstract_content = abstract_header.find_element(By.XPATH, "./following-sibling::div").text
                
                crawled_data.append({
                    "title": title, 
                    "url": link, 
                    "content": abstract_content, 
                    "date": date_str,
                    "source": "HuggingFace Trending",
                    "category": "ê¸°ìˆ "
                })
                print(f"âœ… ì œëª©: {title}")
            except Exception as e:
                print(f"âŒ URL {link} í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                
        print(f"HuggingFace Trending Papers í¬ë¡¤ë§ ì™„ë£Œ! ì´ {len(crawled_data)}ê°œì˜ ë…¼ë¬¸ì„ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤.")
        return crawled_data

    def crawl_aitimes(self):
        """AIíƒ€ì„ì¦ˆ í¬ë¡¤ë§"""
        print("\n=== AIíƒ€ì„ì¦ˆ í¬ë¡¤ë§ ì‹œì‘ ===")
        
        target_url = "https://www.aitimes.com/news/articleList.html?sc_section_code=S1N3&view_type=sm"
        self.driver.get(target_url)

        print("\n'ë”ë³´ê¸°'ë¥¼ í´ë¦­í•˜ì—¬ ê¸°ì‚¬ë¥¼ ì¶”ê°€ë¡œ ë¡œë”©í•©ë‹ˆë‹¤...")
        for i in range(5):
            try:
                wait = WebDriverWait(self.driver, 10)
                more_button = wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, "button.list-btn-more")))
                self.driver.execute_script("arguments[0].click();", more_button)
                print(f"'{i+1}ë²ˆì§¸ 'ë”ë³´ê¸°' í´ë¦­. 2ì´ˆ ëŒ€ê¸°...")
                time.sleep(2)
            except (NoSuchElementException, TimeoutException):
                print("'ë”ë³´ê¸°' ë²„íŠ¼ì´ ì—†ê±°ë‚˜ ë” ì´ìƒ í™œì„±í™”ë˜ì§€ ì•Šì•„ ë¡œë”©ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
                break

        base_url = "https://www.aitimes.com"
        soup = BeautifulSoup(self.driver.page_source, 'html.parser')
        article_links = []
        for tag in soup.select("h2.altlist-subject > a"):
            href = tag.get('href')
            if href:
                full_url = urljoin(base_url, href)
                article_links.append(full_url)
        article_links = list(dict.fromkeys(article_links))
        print(f"\nì´ {len(article_links)}ê°œì˜ ê³ ìœ í•œ ê¸°ì‚¬ ë§í¬ë¥¼ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤. ìƒì„¸ í˜ì´ì§€ í™•ì¸ì„ ì‹œì‘í•©ë‹ˆë‹¤.")

        final_results = []
        for i, url in enumerate(article_links):
            print(f"\n[{i+1}/{len(article_links)}] ê¸°ì‚¬ í™•ì¸ ì¤‘: {url}")
            try:
                self.driver.get(url)
                WebDriverWait(self.driver, 15).until(EC.presence_of_element_located((By.CSS_SELECTOR, "h1.heading")))
                article_soup = BeautifulSoup(self.driver.page_source, 'html.parser')

                date_element = None
                date_type = ""

                date_element = article_soup.select_one("div.info-update-origin")
                date_type = "ì…ë ¥"
                if not date_element:
                    date_element = article_soup.find("li", string=lambda text: text and "ì…ë ¥" in text)
                
                if not date_element:
                    date_element = article_soup.select_one("li.info-update.show")
                    date_type = "ì—…ë°ì´íŠ¸"

                if not date_element:
                    print("-> ì…ë ¥ ë° ì—…ë°ì´íŠ¸ ë‚ ì§œ ì •ë³´ë¥¼ ëª¨ë‘ ì°¾ì§€ ëª»í•´ ê±´ë„ˆëœë‹ˆë‹¤.")
                    continue

                raw_date_text = date_element.get_text(strip=True)
                date_match = re.search(r'(\d{4}\.\d{2}\.\d{2}\s+\d{2}:\d{2})', raw_date_text)
                if not date_match:
                    print(f"-> '{date_type}' ë‚ ì§œ í˜•ì‹ì„ ì°¾ì§€ ëª»í•´ ê±´ë„ˆëœë‹ˆë‹¤: {raw_date_text}")
                    continue
                
                date_str_full = date_match.group(1)
                article_date = datetime.strptime(date_str_full, "%Y.%m.%d %H:%M")
                formatted_date = article_date.strftime('%Y-%m-%d')

                if datetime.now() - article_date <= timedelta(days=7):
                    print(f"-> {date_type} ë‚ ì§œ í™•ì¸: {formatted_date} (ìˆ˜ì§‘ ëŒ€ìƒ)")
                    title = article_soup.select_one("h1.heading").text.strip()
                    content_container = article_soup.select_one("article#article-view-content-div")
                    if content_container:
                        content = "\n".join([p.get_text(strip=True) for p in content_container.find_all("p")])
                    else:
                        content = "ë³¸ë¬¸ ë‚´ìš©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                    
                    final_results.append({
                        "title": title,
                        "url": url,
                        "content": content,
                        "date": formatted_date,
                        "source": "AIíƒ€ì„ì¦ˆ",
                        "category": "ì‚°ì—…"
                    })
                else:
                    print(f"-> {date_type} ë‚ ì§œ í™•ì¸: {formatted_date} (7ì¼ ê²½ê³¼). ìˆ˜ì§‘ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
                    break

            except Exception as e:
                print(f"-> ê¸°ì‚¬ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                continue

        print(f"AIíƒ€ì„ì¦ˆ í¬ë¡¤ë§ ì™„ë£Œ! ì´ {len(final_results)}ê°œì˜ ê¸°ì‚¬ë¥¼ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤.")
        return final_results

    def crawl_arxiv(self, keyword="AI"):
        """arXiv í¬ë¡¤ë§"""
        print(f"\n=== arXiv í¬ë¡¤ë§ ì‹œì‘ (í‚¤ì›Œë“œ: {keyword}) ===")
        
        search_url = "https://arxiv.org/search/advanced"
        one_week_ago = datetime.now() - timedelta(days=7)
        crawled_data = []

        try:
            print(f"ë…¼ë¬¸ ê²€ìƒ‰ì„ ì‹œì‘í•©ë‹ˆë‹¤: '{keyword}'...")
            self.driver.get(search_url)

            print("ê³ ê¸‰ ê²€ìƒ‰ ì˜µì…˜ì„ ì„¤ì •í•©ë‹ˆë‹¤...")
            cs_radio_button = WebDriverWait(self.driver, 10).until(EC.element_to_be_clickable((By.CSS_SELECTOR, "label[for='classification-computer_science']")))
            cs_radio_button.click()
            print("- ì£¼ì œ: Computer Science ì„ íƒë¨")

            select_field = Select(self.driver.find_element(By.NAME, "terms-0-field"))
            select_field.select_by_value("abstract")
            print("- ê²€ìƒ‰ í•„ë“œ: Abstract ì„ íƒë¨")

            search_box = self.driver.find_element(By.NAME, "terms-0-term")
            actions = ActionChains(self.driver)
            actions.move_to_element(search_box).click().send_keys(keyword).perform()
            print(f"- í‚¤ì›Œë“œ '{keyword}' ì…ë ¥ë¨")

            search_button = self.driver.find_element(By.CSS_SELECTOR, "button.button.is-link.is-medium")
            search_button.click()
            print("ê²€ìƒ‰ì„ ì‹¤í–‰í•©ë‹ˆë‹¤.")

            while True:
                WebDriverWait(self.driver, 10).until(EC.presence_of_element_located((By.CSS_SELECTOR, "li.arxiv-result")))
                print(f"\ní˜„ì¬ í˜ì´ì§€ì—ì„œ ë…¼ë¬¸ ì •ë³´ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤ ({self.driver.current_url})...")
                
                papers = self.driver.find_elements(By.CSS_SELECTOR, "li.arxiv-result")
                stop_crawling = False

                for paper in papers:
                    try:
                        date_p_elements = paper.find_elements(By.CSS_SELECTOR, "p.is-size-7")
                        date_text_full = ""
                        for p_element in date_p_elements:
                            if p_element.text.strip().startswith("Submitted"):
                                date_text_full = p_element.text.split('\n')[0]
                                break
                        
                        if not date_text_full:
                            raise NoSuchElementException("Could not find paragraph containing 'Submitted' date information.")

                        paper_date = self.parse_arxiv_date(date_text_full)
                        
                        if paper_date is None or paper_date < one_week_ago:
                            stop_crawling = True
                            print("ì¼ì£¼ì¼ ì´ì „ ë…¼ë¬¸ì— ë„ë‹¬í•˜ì—¬ ìˆ˜ì§‘ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
                            break
                        
                        title = paper.find_element(By.CSS_SELECTOR, "p.title.is-5").text
                        url = paper.find_element(By.CSS_SELECTOR, "p.list-title > a").get_attribute("href")
                        
                        try:
                            abstract_element = paper.find_element(By.CSS_SELECTOR, "span.abstract-full")
                            content = self.driver.execute_script("return arguments[0].textContent;", abstract_element)
                            content = content.replace(" Less", "").strip()
                        except NoSuchElementException:
                            content = ""
                        
                        crawled_data.append({
                            "title": title, 
                            "url": url, 
                            "content": content, 
                            "date": paper_date.strftime("%Y-%m-%d"),
                            "source": "arXiv",
                            "category": "ê¸°ìˆ "
                        })
                        print(f"  - ìˆ˜ì§‘ë¨: {title}")

                    except NoSuchElementException as e:
                        print(f"  - ê²½ê³ : ë‹¤ë¥¸ í˜•ì‹ì˜ í•­ëª©ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
                        continue

                if stop_crawling:
                    break
                
                try:
                    next_button = self.driver.find_element(By.CSS_SELECTOR, "a.pagination-next")
                    next_button.click()
                except NoSuchElementException:
                    print("\në§ˆì§€ë§‰ í˜ì´ì§€ì— ë„ë‹¬í–ˆìŠµë‹ˆë‹¤. í¬ë¡¤ë§ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                    break

        except TimeoutException:
            print("í˜ì´ì§€ ì‹œê°„ ì´ˆê³¼ ë˜ëŠ” ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        except Exception as e:
            print(f"í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")

        print(f"arXiv í¬ë¡¤ë§ ì™„ë£Œ! ì´ {len(crawled_data)}ê°œì˜ ë…¼ë¬¸ì„ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤.")
        return crawled_data

    def parse_arxiv_date(self, date_string):
        """arXiv ë‚ ì§œ ë¬¸ìì—´ì„ datetime ê°ì²´ë¡œ ë³€í™˜"""
        try:
            clean_date_str = date_string.replace('Submitted ', '').split(';')[0].strip()
            return datetime.strptime(clean_date_str, '%d %B, %Y')
        except (ValueError, IndexError):
            return None

    def crawl_aitimes_kr(self):
        """ì¸ê³µì§€ëŠ¥ ì‹ ë¬¸ í¬ë¡¤ë§"""
        print("\n=== ì¸ê³µì§€ëŠ¥ ì‹ ë¬¸ í¬ë¡¤ë§ ì‹œì‘ ===")
        
        list_url = "https://www.aitimes.kr/news/articleList.html?sc_section_code=S1N2&view_type=sm"
        
        print(f"ê¸°ì‚¬ ëª©ë¡ í˜ì´ì§€ë¡œ ì´ë™í•©ë‹ˆë‹¤: {list_url}")
        self.driver.get(list_url)
        
        try:
            WebDriverWait(self.driver, 15).until(lambda d: d.find_elements(By.CSS_SELECTOR, "h4.titles a, a.list-title"))
            print("í˜ì´ì§€ ë¡œë”© ì™„ë£Œ.")
        except TimeoutException:
            print("í˜ì´ì§€ ë¡œë”© ì‹œê°„ ì´ˆê³¼ ë˜ëŠ” ê¸°ì‚¬ ì œëª© ë§í¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return []

        article_links_count = len(self.driver.find_elements(By.CSS_SELECTOR, "h4.titles a, a.list-title"))
        print(f"í˜ì´ì§€ì—ì„œ ì´ {article_links_count}ê°œì˜ ê¸°ì‚¬ ë§í¬ë¥¼ ë°œê²¬í–ˆìŠµë‹ˆë‹¤.")

        final_results = []
        
        for i in range(article_links_count):
            try:
                WebDriverWait(self.driver, 15).until(EC.presence_of_element_located((By.CSS_SELECTOR, "h4.titles a, a.list-title")))
                links = self.driver.find_elements(By.CSS_SELECTOR, "h4.titles a, a.list-title")
                
                link_text = links[i].text
                print(f"\n[{i+1}/{article_links_count}] '{link_text}' ê¸°ì‚¬ë¥¼ í´ë¦­í•©ë‹ˆë‹¤.")
                links[i].click()

                WebDriverWait(self.driver, 15).until(EC.presence_of_element_located((By.CSS_SELECTOR, "h3.heading")))
                article_soup = BeautifulSoup(self.driver.page_source, 'html.parser')
                
                date_info_element = article_soup.select_one("ul.infomation li:nth-child(2)")
                if not date_info_element:
                    print("ìƒì„¸ í˜ì´ì§€ì—ì„œ ë‚ ì§œ ì •ë³´ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
                    self.driver.back()
                    continue

                date_str_full = date_info_element.text.strip().replace("ì…ë ¥ ", "")
                article_date = datetime.strptime(date_str_full, "%Y.%m.%d %H:%M")

                if datetime.now() - article_date <= timedelta(days=7):
                    formatted_date = article_date.strftime('%Y-%m-%d')
                    print(f"-> ë‚ ì§œ í™•ì¸: {formatted_date} (ìˆ˜ì§‘ ëŒ€ìƒ)")
                    
                    title = article_soup.select_one("h3.heading").text.strip()
                    
                    content_container = article_soup.select_one("article#article-view-content-div")
                    if content_container:
                        p_tags = content_container.find_all("p")
                        content = "\n".join([p.get_text(strip=True) for p in p_tags])
                    else:
                        content = "ë³¸ë¬¸ ë‚´ìš©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                    
                    final_results.append({
                        "title": title,
                        "url": self.driver.current_url,
                        "content": content,
                        "date": formatted_date,
                        "source": "ì¸ê³µì§€ëŠ¥ ì‹ ë¬¸",
                        "category": "ê¸°ìˆ "
                    })
                else:
                    formatted_date = article_date.strftime('%Y-%m-%d')
                    print(f"-> ë‚ ì§œ í™•ì¸: {formatted_date} (7ì¼ ê²½ê³¼). ìˆ˜ì§‘ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
                    self.driver.back()
                    break
                
                self.driver.back()

            except StaleElementReferenceException:
                print("í˜ì´ì§€ ìš”ì†Œê°€ ë³€ê²½ë˜ì–´ ê±´ë„ˆëœë‹ˆë‹¤.")
                self.driver.get(list_url)
                continue
            except Exception as e:
                print(f"ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                self.driver.get(list_url)
                continue

        print(f"ì¸ê³µì§€ëŠ¥ ì‹ ë¬¸ í¬ë¡¤ë§ ì™„ë£Œ! ì´ {len(final_results)}ê°œì˜ ê¸°ì‚¬ë¥¼ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤.")
        return final_results

    def search_with_perplexity(self, context: str = "AI") -> list:
        """
        Perplexity APIë¥¼ ì‚¬ìš©í•˜ì—¬ AI ë™í–¥ ë°ì´í„° ìˆ˜ì§‘
        
        Args:
            context (str): ê²€ìƒ‰ ì»¨í…ìŠ¤íŠ¸ (arxiv í‚¤ì›Œë“œì™€ ë™ì¼)
            
        Returns:
            list: ìˆ˜ì§‘ëœ AI ë™í–¥ ë°ì´í„° ë¦¬ìŠ¤íŠ¸
        """
        print(f"\n=== Perplexity APIë¥¼ í†µí•œ AI ë™í–¥ ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘ (ì»¨í…ìŠ¤íŠ¸: {context}) ===")
        
        if not self.perplexity_api_key:
            print("âŒ Perplexity API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return []
        
        # í˜„ì¬ ë‚ ì§œ ê³„ì‚°
        current_date = datetime.now()
        start_date = (current_date - timedelta(days=7)).strftime("%Y-%m-%d")
        end_date = current_date.strftime("%Y-%m-%d")
        
        # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
        prompt_template = f"""
# ìµœì¢… ëª©í‘œ (Primary Goal)
ë‹¹ì‹ ì˜ ìœ ì¼í•œ ì„ë¬´ëŠ” ì•„ë˜ì— ëª…ì‹œëœ êµ¬ì¡°ì™€ ê·œì¹™ì„ ì™„ë²½í•˜ê²Œ ë”°ë¥´ëŠ” ë‹¨ì¼ JSON ê°ì²´ë¥¼ ìƒì„±í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. JSON ê°ì²´ ì™¸ì— ì–´ë– í•œ ì„¤ëª…, ì¸ì‚¬, ì¶”ê°€ í…ìŠ¤íŠ¸ë„ ì¶œë ¥í•´ì„œëŠ” ì•ˆ ë©ë‹ˆë‹¤.

# JSON êµ¬ì¡° ë° ê·œì¹™ (JSON Structure & Rules)
[
    {{
        "title": "ì£¼ê°„ AI ê¸°ìˆ  ë™í–¥ ë³´ê³ ì„œ ({start_date} ~ {end_date})",
        "url": "",
        "content": "{{CONTENT_PLACEHOLDER}}",
        "date": "{current_date.strftime('%Y-%m-%d')}",
        "source": "í¼í”Œë ‰ì‹œí‹°",
        "category": "ì¢…í•©"
    }}
]

# 'content' í•„ë“œ ì‘ì„± ì§€ì¹¨ ({{CONTENT_PLACEHOLDER}}ë¥¼ ì±„ìš°ê¸° ìœ„í•œ ê·œì¹™)
1.  **í•µì‹¬ ì£¼ì œ**: {start_date}ë¶€í„° {end_date}ê¹Œì§€, ì§€ë‚œ 7ì¼ê°„ ë°œìƒí•œ AI ë¶„ì•¼ ë™í–¥ ì¤‘ì—ì„œ **"{context}"ì™€ ì§ì ‘ì ìœ¼ë¡œ ê´€ë ¨ëœ ë‚´ìš©ì„ ìµœìš°ì„ ìœ¼ë¡œ ë‹¤ë£¨ì–´ì•¼ í•©ë‹ˆë‹¤.** "{context}" ê´€ë ¨ ë‰´ìŠ¤, ì—°êµ¬, ê¸°ìˆ  ë°œì „, ê¸°ì—… ë°œí‘œ ë“±ì„ ì „ì²´ ë‚´ìš©ì˜ 70% ì´ìƒì„ ì°¨ì§€í•˜ë„ë¡ êµ¬ì„±í•˜ì„¸ìš”.

2.  **ë¶„ëŸ‰**: ì „ì²´ ê¸€ì˜ ê¸¸ì´ëŠ” í•œêµ­ì–´ ê¸°ì¤€ 1000ì ë‚´ì™¸ë¡œ ë§ì¶°ì£¼ì„¸ìš”.

3.  **í˜•ì‹**: ì—¬ëŸ¬ ì£¼ì œë¥¼ ìì—°ìŠ¤ëŸ½ê²Œ ì—®ì–´ë‚¸ ì¤„ê¸€(prose) í˜•íƒœë¡œ ì‘ì„±í•©ë‹ˆë‹¤. ë§ˆí¬ë‹¤ìš´ í—¤ë”(#)ë‚˜ ê¸€ë¨¸ë¦¬ ê¸°í˜¸(-)ë¥¼ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”.

4.  **ì¶œì²˜**: ì‹ ë¢°ë„ ë†’ì€ ìµœì‹  ê¸°ì‚¬, ë…¼ë¬¸, ë°œí‘œ ë“±ì„ ë°”íƒ•ìœ¼ë¡œ ë‚´ìš©ì„ êµ¬ì„±í•˜ë˜, **ë§¤ìš° ì¤‘ìš”: 'content' ë³¸ë¬¸ ì•ˆì—ëŠ” ì ˆëŒ€ë¡œ URL, ì›¹ì‚¬ì´íŠ¸ ì´ë¦„, ë…¼ë¬¸ ì œëª© ë“± ì¶œì²˜ë¥¼ í‘œê¸°í•˜ì§€ ë§ˆì„¸ìš”.** ëª¨ë“  ì •ë³´ë¥¼ ìì‹ ì˜ ë¶„ì„ì¸ ê²ƒì²˜ëŸ¼ ì¢…í•©í•´ì„œ ì„œìˆ í•´ì•¼ í•©ë‹ˆë‹¤.

5.  **ì»¨í…ìŠ¤íŠ¸ ì§‘ì¤‘**: 
    - "{context}"ì™€ ì§ì ‘ ê´€ë ¨ëœ AI ê¸°ìˆ , ì—°êµ¬, ê¸°ì—… í™œë™ì„ ìš°ì„ ì ìœ¼ë¡œ í¬í•¨
    - "{context}" ê´€ë ¨ ìµœì‹  ë™í–¥ê³¼ ë°œì „ ìƒí™©ì„ ìƒì„¸íˆ ë‹¤ë£¨ê¸°
    - "{context}"ê°€ AI ìƒíƒœê³„ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ê³¼ ì˜ë¯¸ë¥¼ ë¶„ì„
    - "{context}" ê´€ë ¨ ê¸°ì—…ë“¤ì˜ ì „ëµì  ì›€ì§ì„ê³¼ íˆ¬ì ë™í–¥ í¬í•¨
    - "{context}" ê¸°ìˆ ì˜ ì‹¤ì œ ì ìš© ì‚¬ë¡€ì™€ ì„±ê³¼ ë‹¤ë£¨ê¸°

6.  **ë³´ì¡° ë‚´ìš©**: ë‚˜ë¨¸ì§€ 30%ëŠ” "{context}"ì™€ ê°„ì ‘ì ìœ¼ë¡œ ê´€ë ¨ëœ ì „ì²´ì ì¸ AI ë™í–¥ì„ í¬í•¨í•˜ë˜, ê°€ëŠ¥í•œ í•œ "{context}"ì™€ì˜ ì—°ê´€ì„±ì„ ì°¾ì•„ ì—°ê²°í•˜ì—¬ ì„œìˆ í•˜ì„¸ìš”.

7.  **êµ¬ì²´ì„±**: "{context}" ê´€ë ¨ ë‚´ìš©ì€ êµ¬ì²´ì ì¸ ê¸°ìˆ ëª…, ê¸°ì—…ëª…, ì—°êµ¬ ê²°ê³¼, ìˆ˜ì¹˜ ë“±ì„ í¬í•¨í•˜ì—¬ ì‹ ë¢°ì„± ìˆëŠ” ì •ë³´ë¥¼ ì œê³µí•˜ì„¸ìš”.
"""
        
        collected_data = []
        
        # ì²« ë²ˆì§¸ API í˜¸ì¶œ: ì»¨í…ìŠ¤íŠ¸ ì¤‘ì‹¬ AI ë™í–¥
        print("1ï¸âƒ£ ì²« ë²ˆì§¸ API í˜¸ì¶œ: ì»¨í…ìŠ¤íŠ¸ ì¤‘ì‹¬ AI ë™í–¥ ìˆ˜ì§‘ ì¤‘...")
        try:
            general_data = self._call_perplexity_api(prompt_template, f"{context} AI ê¸°ìˆ  ë™í–¥ ìµœì‹  ë‰´ìŠ¤ ì—°êµ¬ ë°œì „")
            if general_data:
                collected_data.extend(general_data)
                print("âœ… ì²« ë²ˆì§¸ API í˜¸ì¶œ ì™„ë£Œ")
        except Exception as e:
            print(f"âŒ ì²« ë²ˆì§¸ API í˜¸ì¶œ ì‹¤íŒ¨: {e}")
        
        # ë‘ ë²ˆì§¸ API í˜¸ì¶œ: ì»¨í…ìŠ¤íŠ¸ íŠ¹í™” ë™í–¥
        print("2ï¸âƒ£ ë‘ ë²ˆì§¸ API í˜¸ì¶œ: ì»¨í…ìŠ¤íŠ¸ íŠ¹í™” ë™í–¥ ìˆ˜ì§‘ ì¤‘...")
        try:
            context_data = self._call_perplexity_api(prompt_template, f"{context} ì¸ê³µì§€ëŠ¥ ê¸°ìˆ  ë°œì „ ê¸°ì—… íˆ¬ì ì—°êµ¬ ë™í–¥ ìµœì‹  ì†Œì‹")
            if context_data:
                collected_data.extend(context_data)
                print("âœ… ë‘ ë²ˆì§¸ API í˜¸ì¶œ ì™„ë£Œ")
        except Exception as e:
            print(f"âŒ ë‘ ë²ˆì§¸ API í˜¸ì¶œ ì‹¤íŒ¨: {e}")
        
        print(f"Perplexity API ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ! ì´ {len(collected_data)}ê°œì˜ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤.")
        return collected_data
    
    def _call_perplexity_api(self, prompt_template: str, search_query: str) -> list:
        """
        Perplexity API í˜¸ì¶œ
        
        Args:
            prompt_template (str): í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
            search_query (str): ê²€ìƒ‰ ì¿¼ë¦¬
            
        Returns:
            list: íŒŒì‹±ëœ ë°ì´í„° ë¦¬ìŠ¤íŠ¸
        """
        try:
            url = "https://api.perplexity.ai/chat/completions"
            
            headers = {
                "Authorization": f"Bearer {self.perplexity_api_key}",
                "Content-Type": "application/json"
            }
            
            # í”„ë¡¬í”„íŠ¸ì— ê²€ìƒ‰ ì¿¼ë¦¬ ì¶”ê°€
            full_prompt = f"{prompt_template}\n\n**ê²€ìƒ‰ ì£¼ì œ**: {search_query}"
            
            payload = {
                "model": "sonar",
                "messages": [
                    {"role": "user", "content": full_prompt}
                ]
            }
            
            print(f"ğŸ” API í˜¸ì¶œ ì¤‘: {search_query[:50]}...")
            response = requests.post(url, json=payload, headers=headers, timeout=60)
            response.raise_for_status()
            
            data = response.json()
            content = data['choices'][0]['message']['content']
            
            # JSON íŒŒì‹±
            parsed_data = self._parse_perplexity_response(content)
            return parsed_data if parsed_data else []
            
        except requests.exceptions.HTTPError as err:
            print(f"âŒ Perplexity API HTTP ì—ëŸ¬: {err} (ìƒíƒœ ì½”ë“œ: {response.status_code})")
            print(f"ì‘ë‹µ ë‚´ìš©: {response.text}")
            return []
        except requests.exceptions.Timeout:
            print("âŒ Perplexity API í˜¸ì¶œ ì‹œê°„ ì´ˆê³¼ (60ì´ˆ)")
            return []
        except requests.exceptions.RequestException as e:
            print(f"âŒ Perplexity API í˜¸ì¶œ ì˜¤ë¥˜: {str(e)}")
            return []
        except Exception as e:
            print(f"âŒ Perplexity API í˜¸ì¶œ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {str(e)}")
            return []
    
    def _parse_perplexity_response(self, response_text: str) -> list:
        """
        Perplexity API ì‘ë‹µì„ íŒŒì‹±í•˜ì—¬ ë°ì´í„° ì¶”ì¶œ
        
        Args:
            response_text (str): API ì‘ë‹µ í…ìŠ¤íŠ¸
            
        Returns:
            list: íŒŒì‹±ëœ ë°ì´í„° ë¦¬ìŠ¤íŠ¸
        """
        try:
            # JSON ì‘ë‹µ ì¶”ì¶œ
            if '[' in response_text and ']' in response_text:
                start = response_text.find('[')
                end = response_text.rfind(']') + 1
                json_str = response_text[start:end]
                parsed_data = json.loads(json_str)
                
                if isinstance(parsed_data, list):
                    return parsed_data
                elif isinstance(parsed_data, dict):
                    return [parsed_data]
                else:
                    return []
            else:
                print("âš ï¸ JSON í˜•ì‹ì˜ ì‘ë‹µì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return []
                
        except json.JSONDecodeError as e:
            print(f"âš ï¸ JSON íŒŒì‹± ì˜¤ë¥˜: {e}")
            print(f"ì‘ë‹µ í…ìŠ¤íŠ¸: {response_text[:200]}...")
            return []
        except Exception as e:
            print(f"âš ï¸ ì‘ë‹µ íŒŒì‹± ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}")
            return []

    def crawl_all_sources(self, arxiv_keyword="AI"):
        """ëª¨ë“  ì†ŒìŠ¤ì—ì„œ í¬ë¡¤ë§ ì‹¤í–‰"""
        print("="*60)
        print("ëª¨ë“  ì†ŒìŠ¤ì—ì„œ í¬ë¡¤ë§ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
        print("="*60)
        
        all_results = []
        
        try:
            # Perplexity APIë¥¼ í†µí•œ AI ë™í–¥ ë°ì´í„° ìˆ˜ì§‘
            perplexity_data = self.search_with_perplexity(arxiv_keyword)
            all_results.extend(perplexity_data)
            
            # íŒŒì´í† ì¹˜ í•œêµ­ ì‚¬ìš©ì ëª¨ì„
            pytorch_data = self.crawl_pytorch_kr()
            all_results.extend(pytorch_data)
            
            # TechCrunch
            techcrunch_data = self.crawl_techcrunch()
            all_results.extend(techcrunch_data)
            
            # HuggingFace Trending
            huggingface_data = self.crawl_huggingface()
            all_results.extend(huggingface_data)
            
            # AIíƒ€ì„ì¦ˆ
            aitimes_data = self.crawl_aitimes()
            all_results.extend(aitimes_data)
            
            # arXiv
            arxiv_data = self.crawl_arxiv(arxiv_keyword)
            all_results.extend(arxiv_data)
            
            # ì¸ê³µì§€ëŠ¥ ì‹ ë¬¸
            aitimes_kr_data = self.crawl_aitimes_kr()
            all_results.extend(aitimes_kr_data)
            
        except Exception as e:
            print(f"í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
        finally:
            self.close_driver()
        
        return all_results

    def save_results(self, data, filename="combined_search_results.json"):
        """ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥"""
        with open(filename, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=4)
        
        print(f"\n" + "="*60)
        print(f"í¬ë¡¤ë§ ì™„ë£Œ! ì´ {len(data)}ê°œì˜ ë°ì´í„°ë¥¼ '{filename}' íŒŒì¼ì— ì €ì¥í–ˆìŠµë‹ˆë‹¤.")
        print("="*60)
        
        # ì¹´í…Œê³ ë¦¬ë³„ í†µê³„ ì¶œë ¥
        categories = {}
        sources = {}
        for item in data:
            cat = item.get('category', 'Unknown')
            src = item.get('source', 'Unknown')
            categories[cat] = categories.get(cat, 0) + 1
            sources[src] = sources.get(src, 0) + 1
        
        print("\nğŸ“Š ì¹´í…Œê³ ë¦¬ë³„ í†µê³„:")
        for cat, count in categories.items():
            print(f"  {cat}: {count}ê°œ")
        
        print("\nğŸ“Š ì†ŒìŠ¤ë³„ í†µê³„:")
        for src, count in sources.items():
            print(f"  {src}: {count}ê°œ")
        
        # Perplexity ë°ì´í„°ê°€ ìˆëŠ”ì§€ í™•ì¸
        perplexity_count = sum(1 for item in data if item.get('source') == 'í¼í”Œë ‰ì‹œí‹°')
        if perplexity_count > 0:
            print(f"\nğŸ¤– Perplexity API ë°ì´í„°: {perplexity_count}ê°œ")

class SearcherAgent(BaseAgent):
    """ì›¹ í¬ë¡¤ë§ ë° ì •ë³´ ìˆ˜ì§‘ ì—ì´ì „íŠ¸"""
    
    def __init__(self, perplexity_api_key: str = None):
        super().__init__(
            name="searcher",
            description="ì›¹ í¬ë¡¤ë§ ë° ì •ë³´ ìˆ˜ì§‘ ì—ì´ì „íŠ¸"
        )
        self.required_inputs = ["query", "arxiv_keyword"]
        self.output_keys = ["search_results", "search_metadata"]
        self.web_searcher = WebSearcher(perplexity_api_key)
    
    async def process(self, state: WorkflowState) -> WorkflowState:
        """ì›¹ í¬ë¡¤ë§ ë° ì •ë³´ ìˆ˜ì§‘ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤."""
        self.log_execution("ì›¹ í¬ë¡¤ë§ ë° ì •ë³´ ìˆ˜ì§‘ ì‹œì‘")
        
        try:
            # ì…ë ¥ ê²€ì¦
            if not self.validate_inputs(state):
                raise ValueError("í•„ìˆ˜ ì…ë ¥ì´ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤.")
            
            # í¬ë¡¤ë§ ì‹¤í–‰
            arxiv_keyword = getattr(state, 'arxiv_keyword', 'AI')
            search_results = self.web_searcher.crawl_all_sources(arxiv_keyword)
            
            # ê²°ê³¼ ì €ì¥
            output_filename = f"agent-cast/output/searcher/search_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            self.web_searcher.save_results(search_results, output_filename)
            
            # ì›Œí¬í”Œë¡œìš° ìƒíƒœ ì—…ë°ì´íŠ¸
            new_state = WorkflowState(
                **{k: v for k, v in state.__dict__.items()},
                search_results=search_results,
                search_metadata={
                    "total_items": len(search_results),
                    "sources": list(set(item.get('source', 'Unknown') for item in search_results)),
                    "categories": list(set(item.get('category', 'Unknown') for item in search_results)),
                    "output_file": output_filename
                }
            )
            
            # ì›Œí¬í”Œë¡œìš° ìƒíƒœ ì—…ë°ì´íŠ¸
            new_state = self.update_workflow_status(new_state, "searcher_completed")
            
            self.log_execution(f"ì›¹ í¬ë¡¤ë§ ì™„ë£Œ: {len(search_results)}ê°œ í•­ëª© ìˆ˜ì§‘")
            return new_state
            
        except Exception as e:
            self.log_execution(f"ì›¹ í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}", "ERROR")
            raise

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    # Perplexity API í‚¤ ì…ë ¥ (ì„ íƒì‚¬í•­)
    perplexity_api_key = input("Perplexity API í‚¤ë¥¼ ì…ë ¥í•˜ì„¸ìš” (ê¸°ë³¸ê°’ ì‚¬ìš©ì‹œ ì—”í„°): ").strip()
    if not perplexity_api_key:
        perplexity_api_key = None
    
    searcher = WebSearcher(perplexity_api_key=perplexity_api_key)
    
    # arXiv ê²€ìƒ‰ í‚¤ì›Œë“œ ì…ë ¥ (ê¸°ë³¸ê°’: "AI")
    arxiv_keyword = input("arXiv ê²€ìƒ‰ í‚¤ì›Œë“œë¥¼ ì…ë ¥í•˜ì„¸ìš” (ê¸°ë³¸ê°’: AI): ").strip()
    if not arxiv_keyword:
        arxiv_keyword = "AI"
    
    # ëª¨ë“  ì†ŒìŠ¤ì—ì„œ í¬ë¡¤ë§ ì‹¤í–‰
    results = searcher.crawl_all_sources(arxiv_keyword)
    
    # ê²°ê³¼ ì €ì¥
    searcher.save_results(results)

if __name__ == "__main__":
    main()
